In the course you learned how to do classificaiton using Fashion MNIST, a data set containing items of clothing. 
There's another, similar dataset called MNIST which has items of handwriting -- the digits 0 through 9.
Write an MNIST classifier that trains to 99% accuracy or above, and does it without a fixed number of epochs 
-- i.e. you should stop training once you reach that level of accuracy.

Some notes:

It should succeed in less than 10 epochs, so it is okay to change epochs= to 10, but nothing larger
When it reaches 99% or greater it should print out the string "Reached 99% accuracy so cancelling training!"
If you add any additional variables, make sure you use the same names as the ones used in the class
I've started the code for you below -- how would you finish it?

import tensorflow as tf
from os import path, getcwd, chdir

# DO NOT CHANGE THE LINE BELOW. If you are developing in a local
# environment, then grab mnist.npz from the Coursera Jupyter Notebook
# and place it inside a local folder and edit the path to that location
path = f"{getcwd()}/../tmp2/mnist.npz"

# GRADED FUNCTION: train_mnist
def train_mnist():
    # Please write your code only where you are indicated.
    # please do not remove # model fitting inline comments.

    class myCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs={}):
            if(logs.get('acc')>0.99):
                print("\nReached 99% accuracy so cancelling training!")
                self.model.stop_training = True

    mnist = tf.keras.datasets.mnist

    (x_train, y_train),(x_test, y_test) = mnist.load_data(path=path)
    # YOUR CODE SHOULD START HERE
    x_train, x_test = x_train/100, x_test/100
    # YOUR CODE SHOULD END HERE
    model = tf.keras.models.Sequential([
        # YOUR CODE SHOULD START HERE
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(512, activation=tf.nn.relu),
        tf.keras.layers.Dense(10, activation=tf.nn.softmax)
        # YOUR CODE SHOULD END HERE
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    
    # model fitting
    callbacks = myCallback()
    history=model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])

    return history.epoch, history.history['acc'][-1]

WARNING: Logging before flag parsing goes to stderr.
W0604 10:24:48.705856 140233786107712 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Epoch 1/10
60000/60000 [==============================] - 14s 230us/sample - loss: 0.1844 - acc: 0.9444
Epoch 2/10
60000/60000 [==============================] - 13s 214us/sample - loss: 0.0805 - acc: 0.9743
Epoch 3/10
60000/60000 [==============================] - 13s 218us/sample - loss: 0.0533 - acc: 0.9825
Epoch 4/10
60000/60000 [==============================] - 13s 219us/sample - loss: 0.0370 - acc: 0.9880
Epoch 5/10
60000/60000 [==============================] - 12s 208us/sample - loss: 0.0309 - acc: 0.9898
Epoch 6/10
59808/60000 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9910
Reached 99% accuracy so cancelling training!
60000/60000 [==============================] - 12s 205us/sample - loss: 0.0268 - acc: 0.9910
([0, 1, 2, 3, 4, 5], 0.9910333)
